coreNodeSelector: &coreNodeSelector
  hub.jupyter.org/node-purpose: core

jupyterhub:
  proxy:
    chp:
      nodeSelector: *coreNodeSelector
    service:
      type: ClusterIP
  ingress:
    enabled: true
    annotations:
      ingress.kubernetes.io/proxy-body-size: 64m
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "true"
    hosts:
      - gfts.minrk.net
    tls:
      - secretName: tls-jupyterhub
        hosts:
          - gfts.minrk.net

  singleuser:
    events: true
    startTimeout: 600
    storage:
      capacity: 100Gi
      homeMountPath: /home/jovyan
    image:
      name: "c63eqfuv.c1.gra9.container-registry.ovh.net/gfts/jupyterhub-user"
      tag: "set-by-chartpress"
      pullPolicy: Always
    memory:
      limit: 24G
      guarantee: 8G
    cpu:
      limit: 6
      guarantee: 2
    extraEnv:
      CULL_CONNECTED: "1"
      CULL_TIMEOUT: "1800"
      CULL_KERNEL_TIMEOUT: "1800"
      CULL_INTERVAL: "120"
      GH_SCOPED_CREDS_CLIENT_ID: Iv1.f4a7db20c671f599
      GH_SCOPED_CREDS_APP_URL: https://github.com/apps/gfts-jupyterhub
      AWS_ENDPOINT_URL_S3: "https://s3.gra.perf.cloud.ovh.net"
      AWS_DEFAULT_REGION: gra
      # JUPYTER_FS_BUCKETS: destine-gfts-data-lake,gfts-reference-data,gfts-ifremer
    profileList:
      - display_name: "GFTS environment"
        slug: default
        description: Select resource allocation
        default: true
        profile_options:
          resource_allocation:
            display_name: Resource allocation
            choices:
              default:
                default: true
                display_name: Default (24 GB / 2-6 CPU)
                kubespawner_override:
                  mem_guarantee: 8G
                  mem_limit: 24G
                  cpu_guarantee: 2
                  cpu_limit: 6
              big60:
                display_name: 60 GB / 8 CPU
                kubespawner_override:
                  mem_guarantee: 60e9
                  mem_limit: 60e9
                  cpu_guarantee: 7
                  cpu_limit: 8
              big120:
                display_name: 120 GB / 15 CPU
                kubespawner_override:
                  mem_guarantee: 120e9
                  mem_limit: 120e9
                  cpu_guarantee: 15
                  cpu_limit: 15
              big240:
                display_name: 240 GB / 30 CPU
                kubespawner_override:
                  mem_guarantee: 240e9
                  mem_limit: 240e9
                  cpu_guarantee: 30
                  cpu_limit: 30
              huge:
                display_name: Huge (480 GB / 60 CPU)
                kubespawner_override:
                  mem_guarantee: 480e9
                  mem_limit: 480e9
                  cpu_guarantee: 60
                  cpu_limit: 60
                  node_selector:
                    gfts.destination-earth.eu/size: big512

    extraFiles:
      allow-iframe:
        mountPath: /etc/jupyter/jupyter_server_config.json
        data:
          ServerApp:
            tornado_settings:
              headers:
                Content-Security-Policy: frame-ancestors 'self'
    initContainers:
      # seems to be something in the volume mount resetting permissions
      # .ssh doesn't like to be public, so ensure it's private
      - name: ssh-permissions-and-rclone-config
        image: busybox:1.36
        command:
          - sh
          - "-c"
          - |
            if [ -d /homedir/.ssh ]; then
              chmod -R og= /homedir/.ssh
            fi
            mkdir -p /homedir/.config/rclone
            cat <<EOF > /homedir/.config/rclone/rclone.conf
            [cmarine]
            type = s3
            provider = Other
            endpoint = https://s3.waw3-1.cloudferro.com
            acl = public-read

            [gfts]
            type = s3
            provider = Other
            env_auth = true
            region = gra
            endpoint = https://s3.gra.perf.cloud.ovh.net
            EOF
        volumeMounts:
          - mountPath: /homedir
            name: volume-{username}

  prePuller:
    hook:
      enabled: false
    continuous:
      enabled: true

  hub:
    nodeSelector: *coreNodeSelector
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/path: /hub/metrics
    config:
      JupyterHub:
        authenticate_prometheus: false
      Authenticator:
        allowed_users:
          - minrk
          - tinaok
          - yellowcap
          - danielfdsilva
          - annefou
          - aderrien7
          - jmdelouis
          - mwoillez
          - marinerandon
          - keewis
          - QuentinMaz
      KubeSpawner:
        pod_security_context:
          fsGroup: 100
          fsGroupChangePolicy: OnRootMismatch

    extraConfig:
      load_creds: |
        import json
        from pathlib import Path
        creds_path = Path("/srv/s3creds.json")

        def load_creds(spawner):
            with creds_path.open() as f:
                all_creds = json.load(f)
            name = spawner.user.name
            if name in all_creds:
                spawner.log.info(f"Loading AWS credentials for {name}")
                creds = all_creds[name]
            else:
                spawner.log.info(f"Using default AWS credentials for {name}")
                creds = all_creds["_default"]
            spawner.environment["AWS_ACCESS_KEY_ID"] = creds["aws_access_key_id"]
            spawner.environment["AWS_SECRET_ACCESS_KEY"] = creds["aws_secret_access_key"]
          
        c.KubeSpawner.pre_spawn_hook = load_creds
    loadRoles:
      management:
        scopes:
          - admin-ui
          - access:servers
          - list:users
          - read:users
          - servers
          - groups
        users:
          - annefou
          - minrk

  scheduling:
    userScheduler:
      enabled: true
    podPriority:
      enabled: true
    userPlaceholder:
      enabled: true
      replicas: 0
    userPods:
      nodeAffinity:
        # 'prefer' lets users run on small core nodes
        # to save cost
        # 'require' protects core nodes from user resource consumption
        matchNodePurpose: require
    corePods:
      nodeAffinity:
        matchNodePurpose: require

grafana:
  nodeSelector: *coreNodeSelector
  grafana.ini:
    auth.anonymous:
      enabled: true
      org_name: Main Org.
      org_role: Viewer
    auth.basic:
      enabled: true
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "true"
    hosts:
      - grafana.gfts.minrk.net
    tls:
      - hosts:
          - grafana.gfts.minrk.net
        secretName: tls-grafana
  persistence:
    size: 2Gi
    enabled: true
    accessModes:
      - ReadWriteOnce
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: prometheus
          orgId: 1
          type: prometheus
          url: https://prometheus..minrk.net
          isDefault: true
          editable: false

prometheus:
  nodeSelector: *coreNodeSelector
  nodeExporter:
    updateStrategy:
      type: RollingUpdate
  alertmanager:
    enabled: false
  pushgateway:
    enabled: false
  rbac:
    create: true
  server:
    nodeSelector: *coreNodeSelector
    podLabels:
      hub.jupyter.org/network-access-hub: "true"
    ingress:
      enabled: true
      annotations:
        kubernetes.io/ingress.class: nginx
        kubernetes.io/tls-acme: "true"
      hosts:
        - prometheus.gfts.minrk.net
      tls:
        - hosts:
            - prometheus.gfts.minrk.net
          secretName: tls-prometheus
  kube-state-metrics:
    metricLabelsAllowlist:
      - pods=[app,component]
      - nodes=[*]

ingress-nginx:
  rbac:
    create: true
  statsExporter:
    service:
      annotations:
        prometheus.io/scrape: "true"
  controller:
    nodeSelector: *coreNodeSelector
    replicaCount: 2
    scope:
      enabled: true
    config:
      # Allow POSTs of up to 64MB, for large notebook support.
      proxy-body-size: 64m
    stats:
      enabled: true
    service:
      # Preserve client IPs
      externalTrafficPolicy: Local

dask-gateway:
  gateway:
    extraConfig:
      resource-limits: |
        # for now, limit resources to very small clusters
        c.ClusterConfig.cluster_max_cores = 8
        c.ClusterConfig.cluster_max_memory = "8 G"

        # shutdown idle clusters after one hour
        c.ClusterConfig.idle_timeout = 3600
