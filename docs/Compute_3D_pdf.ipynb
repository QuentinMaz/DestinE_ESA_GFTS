{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c535925e-793d-41be-a989-4fae4cdaaa67",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "#  Computing 3D PDF from State matrix and tag pressure information\n",
    "\n",
    "## 1. **Configure the Notebook:**\n",
    "Prepare the notebook to compute the 3D PDF.\n",
    "\n",
    "In this step, we set up the notebook environment for analysis. It includes installing necessary packages, importing required libraries, setting up parameters, and configuring the cluster for distributed computing. It also retrieves the tag data needed for analysis.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f669836f-066c-44f0-aefa-5326b1ff2a73",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Run the following 3 lines of command to install pangeo-fish in the pangeo environment.\n",
    "# Note: These commands install required packages for the analysis.  \n",
    "# You may need to restart your kernel before executing the next cell.\n",
    "# might not need this 2\n",
    "!pip install rich dask_image zstandard xmovie\n",
    "!pip install git+https://github.com/iaocea/xarray-healpy\n",
    "# Step to pip install pangeo-fish , we re-use pangeo-fish functions for this.\n",
    "!pip install -e ../.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea188b-a3b6-4d00-9cf4-7c99c811f809",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules.\n",
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "from pangeo_fish.io import open_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dd510f-d3d2-4474-a983-2ebac4f13b45",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set up execution parameters for the analysis.\n",
    "# Note: This cell is tagged as parameters, allowing automatic updates when configuring with papermill.\n",
    "# `tag_name` corresponds to the name of the biologging tag (DST identification number), \n",
    "\n",
    "# which is also a path for storing all the information for the specific fish tagged with `tag_name`.\n",
    "# `tag_root` specifies the root URL for tag data used for this computation.\n",
    "tag_name = \"A19124\"  \n",
    "\n",
    "# `scratch_root` specifies the root directory for storing output files.\n",
    "tag_root = \"https://data-taos.ifremer.fr/data_tmp/cleaned/tag/\"\n",
    "\n",
    "# `storage_options` specifies options for the filesystem storing output files.\n",
    "scratch_root = \"s3://destine-gfts-data-lake/demo\"\n",
    "\n",
    "# If you are using a local file system, activate the following two lines:\n",
    "storage_options = {\n",
    "    'anon': False, \n",
    "    'profile' : \"gfts\",\n",
    "    'client_kwargs': {\n",
    "        \"endpoint_url\": \"https://s3.gra.perf.cloud.ovh.net\",\n",
    "        \"region_name\": \"gra\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# scratch_root = \".\"\n",
    "# storage_options = None\n",
    "\n",
    "\n",
    "# Default chunk value for the time dimension. This value depends on the configuration of your Dask cluster.\n",
    "chunk_time=24\n",
    "\n",
    "\n",
    "# Parameters for step 2: **Compare Reference Model with DST Information:**\n",
    "# `bbox`, the bounding box, defines the latitude and longitude range for the analysis area.\n",
    "# Define target root directories for storing analysis results.\n",
    "bbox = {\"latitude\": [46, 51], \"longitude\": [-8, -1]} \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a19e76-606c-4a2f-a93f-b5d2c990823f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the default chunk size for optimization.\n",
    "target_root = f\"{scratch_root}/{tag_name}\"\n",
    "\n",
    "# Set up a local cluster for distributed computing.\n",
    "default_chunk = {\"time\": chunk_time, \"lat\": -1, \"lon\": -1}\n",
    "default_chunk_xy = {\"time\": chunk_time, \"x\": -1, \"y\": -1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268b5c0-b1e8-4d12-b6c9-b3b7aa54f99b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open and retrieve the tag data required for the analysis.\n",
    "from distributed import LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206aeb3c-9684-4eac-80e8-e94939529747",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "tag = open_tag(tag_root, tag_name)\n",
    "tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524fe17c-43b2-498b-a06b-91ddfba27b81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "## 2. **Compute pdf_depth**\n",
    "\n",
    "In this step, we compute the PDF only in depth for each time step used with the model. First, we load the reference model to choose the time bins used for creating the depth PDF (we can use the state matrix for that too).\n",
    "\n",
    "**Note:** Here, the maximum depth is fixed at 42, and the interval is set at 2. This can be made 'automatic' or fixed according to the depth levels of our future climate data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c15fd79-f63e-4546-8ebb-c822883f32e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop tag data outside the tagged events interval.\n",
    "from pangeo_fish.cf import bounds_to_bins\n",
    "from pangeo_fish.tags import  reshape_by_bins, to_time_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae27dd-6fcb-4761-b2cf-04055fb85708",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "time_slice = to_time_slice(tag[\"tagging_events/time\"])\n",
    "tag_log = tag[\"dst\"].ds.sel(time=time_slice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc96491-caaf-4bc7-b03b-b31db6766d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save probability distribution, state matrix.\n",
    "states=xr.open_zarr(\n",
    "    f\"{target_root}/states.zarr\")\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2106d-a618-4a0d-97b9-3533ee55d3f1",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Reshape the tag log so that it bins to the time step of the reference model.\n",
    "reshaped_tag = reshape_by_bins(\n",
    "    tag_log,\n",
    "    dim=\"time\",\n",
    "    bins=(\n",
    "        states.cf.add_bounds([\"time\"], output_dim=\"bounds\")\n",
    "        .pipe(bounds_to_bins, bounds_dim=\"bounds\")\n",
    "        .get(\"time_bins\")\n",
    "    ),\n",
    "    bin_dim=\"bincount\",\n",
    "    other_dim=\"obs\",\n",
    ").chunk({\"time\": chunk_time})\n",
    "reshaped_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91412a57-2261-498f-9cd4-553cf54ff352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "maxdepth=42\n",
    "interval=2\n",
    "bins=np.arange(0, maxdepth, interval)\n",
    "def compute_pdf(data,bins=np.arange(0, maxdepth, interval)):\n",
    "\n",
    "    data = data[~np.isnan(data)]\n",
    "    \n",
    "# Remove NaN values.\n",
    "    hist, bin_edges = np.histogram(data, bins=bins, density=True)\n",
    "\n",
    "# Calculate the histogram.\n",
    "    bin_width = bin_edges[1] - bin_edges[0]\n",
    "    pdf = hist * bin_width\n",
    "    return pdf#, bin_edges\n",
    "\n",
    "def compute_pdf_bins(bins=bins):\n",
    "# Normalize the histogram to ensure the sum of the PDF is 1.\n",
    "    data = np.full(reshaped_tag.obs.size, 1.0) # here i make fake dataset just to compute the bins and center of bins\n",
    "\n",
    "    hist, bin_edges = np.histogram(data, bins=bins, density=True)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    return  bin_edges,bin_centers\n",
    "bin_edges,bin_centers = compute_pdf_bins()\n",
    "\n",
    "depth_pdf=(\n",
    "    xr.apply_ufunc(\n",
    "        compute_pdf,  # the function\n",
    "        reshaped_tag['pressure'],\n",
    "        input_core_dims=[[\"obs\"]],\n",
    "        output_core_dims=[[\"depth\"]],\n",
    "        exclude_dims=set((\"obs\",)),\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "        output_dtypes=[reshaped_tag.pressure.dtype] ,\n",
    "        dask_gufunc_kwargs={'output_sizes': \n",
    "                            {\"depth\": bin_centers.size,}}\n",
    "    ).assign_attrs({'long_name': 'depth_pdf'})\n",
    "    .to_dataset(name='depth_pdf')\n",
    "    .assign_coords(depth=bin_centers)\n",
    ")\n",
    "\n",
    "depth_pdf\n",
    "depth_pdf.depth_pdf.plot(x='time',y='depth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc0bbb-9297-4dd9-bfdb-274c47681060",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f572df9b-6fff-4263-ad7b-5a9f5e7b4fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Remove NaN values.\n",
    "depth_pdf.compute().to_zarr(\n",
    "    f\"{target_root}/depth_pdf.zarr\", mode=\"w\", consolidated=True,  \n",
    "        storage_options=storage_options                \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3465001b-6a0d-47e0-9d8c-e94fc6935f19",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "## 3. **Compute 3D PDF**\n",
    "\n",
    "Multiply the 2D map with the depth PDF to obtain the 3D PDF.  Plot and veirify the result.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de72bc34-5af8-45d4-a369-bcf9c6704ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=states.states.chunk(chunks={\"time\":24*2}) * depth_pdf.depth_pdf.chunk(chunks={\"time\":24*2})\n",
    "ds=(\n",
    "    ds\n",
    "    .persist()\n",
    "    .assign_attrs({'long_name': '3D_pdf'}).to_dataset(name='3d_pdf')\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d75e225-7026-41a9-9af2-30e032b658e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate the histogram.\n",
    "(ds\n",
    "\n",
    " .to_zarr(\n",
    "    f\"{target_root}/three_d_pdf.zarr\", mode=\"w\", consolidated=True,  \n",
    "        storage_options=storage_options                \n",
    ")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9b4ec8-cdfb-41c3-afe3-c97d6da63410",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.hvplot.quadmesh(x='longitude',y='latitude',groupby=['time','depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf9d61-470d-434d-aecc-19375a8c8113",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['3d_pdf'].sum(dim=['x','y']).plot(x='time',y='depth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
